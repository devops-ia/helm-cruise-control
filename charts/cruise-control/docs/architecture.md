# Architecture

## Overview

This document explains the architectural decisions behind the Cruise Control Helm Chart, focusing on the separation between server and UI components, multi-cluster support with cluster groups, and the organizational structure of templates and configurations.

## Architecture decision 1: Separate docker images

### Decision

The project builds **two separate Docker images** with distinct responsibilities:

1. **cruise-control**: Server-only image containing the Cruise Control Java application
2. **cruise-control-ui**: UI-only image containing the web interface served by nginx

### Rationale

#### 1. Separation of concerns

Each image has a single, well-defined purpose:

- **cruise-control image**: Runs the Cruise Control server
  - Based on Amazon Corretto (JDK 17)
  - Contains only the JAR files and configuration
  - Exposes port 9090
  - Runs as non-root user `nobody` (UID 99)
  - Size: ~800MB (JDK + application)

- **cruise-control-ui image**: Serves the static web UI with reverse proxy
  - Based on nginx:1.25-alpine
  - Contains HTML/CSS/JS static files
  - Exposes port 80
  - Runs as non-root user `nginx` (UID 101)
  - Size: ~25MB (nginx + static files)

#### 2. Independent scaling

The UI and server can scale independently based on their specific needs:

```yaml
# Scale server based on Kafka cluster load
cruise-control:
  replicaCount: 3

# Scale UI for high user traffic with HPA
ui:
  replicaCount: 2
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
```

#### 3. Resource efficiency

- **UI**: Extremely lightweight, minimal CPU/memory requirements (50m CPU, 64Mi memory)
- **Server**: Resource-intensive, requires significant memory for Kafka metrics processing (500m CPU, 1Gi memory)
- No need to bundle both in a single image when they have different resource profiles

#### 4. Deployment flexibility

Organizations can choose their deployment strategy:

- Deploy only the server (API-only, no UI)
- Deploy UI separately on CDN or edge locations
- Use the Helm chart's integrated reverse proxy for unified deployment
- Disable UI completely with `ui.enabled: false`

#### 5. Security boundaries

- UI runs as non-root user (UID 101) with restrictive nginx config
- Server runs as non-root user (UID 99) with minimal privileges
- Separate ServiceAccounts for fine-grained RBAC
- Different security contexts and network policies for each component

#### 6. Simplified updates

- Update UI independently (new features, bug fixes) without server changes
- Update server independently without rebuilding UI
- Faster CI/CD pipelines (only build what changed)
- Separate versioning for server and UI images

## Architecture decision 2: Multi-cluster support with groups

### Context

The Helm chart supports managing multiple Kafka clusters from a single deployment, with an optional `group` field to organize clusters into logical categories.

### Cluster group feature

Each cluster can optionally define a `group` field that categorizes it:

```yaml
clusters:
  cluster-dev:
    group: "develop"    # Development environment
  cluster-staging:
    group: "staging"    # Staging environment
  cluster-prod:
    # No group specified, defaults to "cluster"
```

**Key characteristics:**

- If `group` is not specified, it defaults to `"cluster"`
- The group is reflected in the UI's `config.csv` file
- Groups help organize clusters in the UI by environment or purpose
- No functional impact on server behavior, purely organizational

**Generated config.csv:**

```csv
develop,cluster-dev,/cluster-dev/kafkacruisecontrol/
staging,cluster-staging,/cluster-staging/kafkacruisecontrol/
cluster,cluster-prod,/cluster-prod/kafkacruisecontrol/
```

### Benefits

#### 1. Environment separation

Organizations typically manage multiple Kafka environments:

- Development clusters (dev, qa, test)
- Staging clusters (staging, pre-prod)
- Production clusters (prod, prod-dr)

The `group` field provides visual organization in the UI without requiring separate Cruise Control deployments.

#### 2. Operational clarity

Groups make it immediately clear which environment you're working with:

- Reduces risk of accidentally running operations on wrong cluster
- Provides visual cues in the UI
- Helps with documentation and training

#### 3. Backward compatibility

- Default group `"cluster"` maintains compatibility with existing deployments
- Optional field means existing configurations work without changes
- No breaking changes to the API or behavior

#### 4. Future extensibility

The group field can be extended in the future for:

- Group-based RBAC policies
- Group-specific monitoring dashboards
- Automated policy enforcement by group

## Architecture decision 3: Template organization

### Decision

Templates are organized into two directories:

```text
templates/
├── server/          # Server component manifests
│   ├── configmap.yaml
│   ├── deployment.yaml
│   ├── hpa.yaml
│   ├── networkpolicy.yaml
│   ├── pdb.yaml
│   ├── secret.yaml
│   ├── service.yaml
│   └── serviceaccount.yaml
├── ui/              # UI component manifests
│   ├── configmap.yaml
│   ├── deployment.yaml
│   ├── hpa.yaml
│   ├── ingress.yaml
│   ├── networkpolicy.yaml
│   ├── pdb.yaml
│   ├── service.yaml
│   └── serviceaccount.yaml
└── _helpers.tpl     # Shared helper functions
```

### Why this organization

#### 1. Clear separation of concerns

- Server manifests are isolated from UI manifests
- Easy to identify which component a manifest belongs to
- Reduces cognitive load when navigating the chart

#### 2. Simplified maintenance

- Changes to server don't accidentally affect UI and vice versa
- Easier code reviews (clear scope of changes)
- Reduces merge conflicts in teams

#### 3. Component-specific configurations

Each directory contains manifests specific to that component:

- **Server**: Heavy focus on Kafka connectivity, metrics, resources
- **UI**: Focus on ingress, reverse proxy, lightweight resources

#### 4. Independent evolution

- Server can add new features without touching UI manifests
- UI can evolve independently (e.g., new ingress controllers)
- Clear boundaries for breaking changes

## Architecture decision 4: Dedicated helpers for UI

### Decision

Create separate Helm helper functions for UI components that automatically include the `app.kubernetes.io/component: ui` label.

### Helpers created

```yaml
cruise-control.ui.serviceAccountName    # UI ServiceAccount name
cruise-control.ui.selectorLabels        # Labels for selectors (includes component: ui)
cruise-control.ui.labels                # Full labels (includes component: ui)
cruise-control.ui.patchselectorLabels   # Topology spread constraint patching
cruise-control.ui.patchTopologySpreadConstraints  # Auto-patch selectors
```

### Advantages

#### 1. Consistent labeling

The `app.kubernetes.io/component` label is automatically included in all UI resources:

```yaml
# Before: Manual annotation (error-prone)
metadata:
  labels:
    app.kubernetes.io/component: ui  # Could be forgotten

# After: Automatic from helper
metadata:
  labels:
    {{- include "cruise-control.ui.labels" . | nindent 4 }}
```

#### 2. Reduced duplication

- No need to manually add `component: ui` in every manifest
- Single source of truth for UI labels
- Easier to change label structure in the future

#### 3. Improved maintainability

- Helper functions are testable and documented
- Changes to label schema affect all resources automatically
- Reduces risk of inconsistent labels across resources

#### 4. Network policy compatibility

Network policies can reliably select UI pods:

```yaml
spec:
  podSelector:
    matchLabels:
      {{- include "cruise-control.ui.selectorLabels" . | nindent 6 }}
```

#### 5. Mirror server pattern

Server components already had dedicated helpers, UI now follows the same pattern for consistency.

## Architecture decision 5: Comprehensive values.yaml structure

### Decision

The `values.yaml` follows a clear structure with server and UI as separate top-level keys:

```yaml
# Server configuration (top-level)
replicaCount: 1
image: {...}
serviceAccount: {...}
service: {...}
resources: {...}
autoscaling: {...}
networkPolicy: {...}
podDisruptionBudget: {...}

# UI configuration (dedicated section)
ui:
  enabled: true
  replicaCount: 1
  image: {...}
  serviceAccount: {...}
  service: {...}
  ingress: {...}
  resources: {...}
  autoscaling: {...}
  networkPolicy: {...}
  podDisruptionBudget: {...}

# Cluster definitions
clusters:
  cluster-name:
    group: "develop"
    config: {...}
    cluster: {...}

# Shared defaults
clustersDefaults:
  config: {...}
```

### Key principles

#### 1. Clear ownership

- Top-level keys are for server (backward compatible)
- `ui.*` keys are for UI component
- No ambiguity about which component a setting affects

#### 2. Feature parity

Both server and UI support the same features:

- ServiceAccounts with annotations
- HorizontalPodAutoscalers
- PodDisruptionBudgets
- NetworkPolicies
- Resource limits and requests

This symmetry makes the chart predictable and easier to learn.

#### 3. Independent enablement

```yaml
# Deploy only server (API-only mode)
ui:
  enabled: false

# Deploy both (default)
ui:
  enabled: true
```

#### 4. Cluster configuration flexibility

Clusters are defined separately from components:

- Multiple clusters share the same server/UI deployment
- Each cluster can have unique configuration
- Defaults can be overridden per-cluster

## Implementation details

### Component structure

```text
┌─────────────────────────────────────────────────────┐
│                    Ingress                          │
│         (cruise-control.example.com)                │
└─────────────────┬───────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────┐
│              UI Service                             │
│         (cruise-control-ui)                         │
│                                                     │
│  ┌────────────────────────────────────────────┐   │
│  │  cruise-control-ui image (nginx:alpine)    │   │
│  │  - Serves static UI files (~25MB)          │   │
│  │  - Routes API requests to server pods      │   │
│  │  - Port 80 (non-root user UID 101)         │   │
│  │  - Grouped cluster list in config.csv      │   │
│  └────────────────────────────────────────────┘   │
└──────────┬────────────────────┬────────────────────┘
           │                    │
           ▼                    ▼
┌──────────────────┐  ┌──────────────────┐
│  cluster-dev     │  │  cluster-prod    │
│  Server Pod      │  │  Server Pod      │
│  (group:develop) │  │  (group:cluster) │
│                  │  │                  │
│  cruise-control  │  │  cruise-control  │
│  image (~800MB)  │  │  image (~800MB)  │
│  Port 9090       │  │  Port 9090       │
│  UID 99          │  │  UID 99          │
└──────────────────┘  └──────────────────┘
```

### Image comparison

| Aspect | cruise-control | cruise-control-ui |
|--------|----------------|-------------------|
| Base image | amazoncorretto:17 | nginx:1.25-alpine |
| Size | ~800MB | ~25MB |
| Purpose | API server | Web UI + reverse proxy |
| Port | 9090 | 80 |
| User | nobody (99) | nginx (101) |
| CPU request | 500m | 50m |
| Memory request | 1Gi | 64Mi |
| Scaling pattern | Based on Kafka load | Based on user traffic |
| HPA support | Optional | Optional |
| Multi-stage build | Yes (build → runtime) | Yes (build → runtime) |

### ServiceAccount separation

Each component has its own ServiceAccount:

```yaml
# Server ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cruise-control-release-name
  labels:
    {{- include "cruise-control.labels" . | nindent 4 }}

# UI ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cruise-control-release-name-ui
  labels:
    {{- include "cruise-control.ui.labels" . | nindent 4 }}
```

**Benefits:**

- Fine-grained RBAC: UI doesn't need Kafka access, server doesn't need ingress access
- Audit trail: Clear separation of component actions
- Security: Principle of least privilege applied per-component

### Dynamic configuration

The `config.csv` file is automatically generated from Helm values:

```yaml
data:
  config.csv: |
    {{- range $clusterName, $clusterConfig := .Values.clusters }}
    {{- $group := $clusterConfig.group | default "cluster" }}
    {{ $group }},{{ $clusterName }},/{{ $clusterName }}/kafkacruisecontrol/
    {{- end }}
```

This ensures:

- UI always reflects actual cluster configuration
- No manual maintenance of cluster list
- Groups are automatically included
- No drift between Helm values and UI configuration

## Security considerations

### Pod security standards

Both components are fully compliant with restricted Pod Security Standards:

#### Server security context

```yaml
securityContext:
  runAsNonRoot: true
  runAsUser: 99
  runAsGroup: 99
  allowPrivilegeEscalation: false
  capabilities:
    drop: [ALL]
  readOnlyRootFilesystem: false

podSecurityContext:
  fsGroup: 99
  runAsNonRoot: true
  seccompProfile:
    type: RuntimeDefault
```

#### UI security context

```yaml
securityContext:
  runAsNonRoot: true
  runAsUser: 101
  runAsGroup: 101
  allowPrivilegeEscalation: false
  capabilities:
    drop: [ALL]
  readOnlyRootFilesystem: false

podSecurityContext:
  fsGroup: 101
  runAsNonRoot: true
  seccompProfile:
    type: RuntimeDefault
```

### Network policies

The architecture supports fine-grained network policies:

```yaml
# UI NetworkPolicy: Only accept traffic from ingress
spec:
  podSelector:
    matchLabels:
      {{- include "cruise-control.ui.selectorLabels" . | nindent 6 }}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
      - namespaceSelector: {}
        podSelector:
          matchLabels:
            app.kubernetes.io/name: ingress-nginx
  egress:
    - to:
      - podSelector:
          matchLabels:
            app.kubernetes.io/component: server

# Server NetworkPolicy: Only accept from UI and access Kafka
spec:
  podSelector:
    matchLabels:
      {{- include "cruise-control.selectorLabels" . | nindent 6 }}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app.kubernetes.io/component: ui
  egress:
    - to:
      - namespaceSelector: {}
      ports:
      - protocol: TCP
        port: 9092  # Kafka
      - protocol: TCP
        port: 2181  # Zookeeper
```

## Performance

### Latency impact

The reverse proxy adds minimal latency:

- **Static content**: Served directly from nginx (`< 1ms` overhead)
- **API requests**: Single proxy hop (`~2-5ms` overhead)
- **Long-running operations**: No impact (streaming responses)

### Scalability

Both components can be scaled independently:

```yaml
# Scale server horizontally
replicaCount: 3

# Scale UI with HPA
ui:
  replicaCount: 2
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
```

### Caching strategy

Static assets are cached aggressively by nginx:

```nginx
location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
    expires 1y;
    add_header Cache-Control "public, immutable";
}
```

## Alternatives considered

### Alternative 1: Single combined image

**Description**: Bundle both server and UI in one Docker image

**Rejected because:**

- Unnecessarily large image size (~800MB) for UI-only deployments
- Cannot scale UI and server independently
- Every server update requires UI rebuild and vice versa
- Mixed resource requirements complicate resource allocation
- Different security contexts cannot be applied separately
- Violates single responsibility principle

### Alternative 2: Flat template structure

**Description**: Keep all manifests in `templates/` root directory

**Rejected because:**

- Hard to distinguish server from UI manifests
- Increases risk of accidental cross-component changes
- Poor developer experience navigating files
- No clear ownership of manifests

### Alternative 3: Manual component labels

**Description**: Require users to manually add `component: ui` labels

**Rejected because:**

- Error-prone (easy to forget)
- Inconsistent across resources
- Harder to maintain
- Helpers provide automation and consistency

### Alternative 4: Multiple ingress resources

**Description**: Create separate Ingress per cluster

**Rejected because:**

- Fragments the user experience
- Requires multiple DNS entries
- No unified UI
- Increased operational overhead
- No cluster grouping possible

### Alternative 5: No cluster groups

**Description**: Omit the `group` field entirely

**Rejected because:**

- Lacks organizational clarity for multi-environment setups
- No way to visually distinguish environments in UI
- Missed opportunity for future group-based features
- Default value maintains backward compatibility anyway

## Conclusion

The architectural decisions outlined in this document provide a robust, scalable, and maintainable solution for deploying Cruise Control:

1. **Separate docker images**: Server and UI are independently deployable, scalable, and updatable
2. **Multi-cluster with groups**: Flexible cluster organization with optional grouping for clarity
3. **Organized templates**: Clear separation between server and UI manifests
4. **Dedicated helpers**: Automated, consistent labeling for UI components
5. **Comprehensive configuration**: Feature parity between server and UI
6. **Resource efficiency**: Each component sized appropriately for its workload
7. **Security first**: Non-root users, minimal privileges, separate ServiceAccounts
8. **Operational excellence**: Predictable patterns, simplified troubleshooting

This design enables teams to:

- Start small with a single cluster and seamlessly scale to multiple clusters
- Organize clusters into logical groups (develop, staging, production)
- Scale components independently based on their specific needs
- Maintain clear separation of concerns across the stack
- Apply security policies per-component
- Operate with confidence using consistent patterns

The optional `group` field provides organizational clarity without adding complexity, defaulting to `"cluster"` for backward compatibility while enabling enhanced multi-environment workflows.
